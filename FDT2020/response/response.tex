\documentclass[11pt,epsf]{article}

%\documentstyle[12pt,psfig,draft]{article}
%% These ``comments'' only appear with [draft] option
\long\def\comment#1{\ifdim\overfullrule>0pt{\sf[{#1}]}\fi}
\def\mn{\medskip}

\usepackage{amsmath}
\usepackage{comment,amsthm,graphicx}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage{bbm}
\usepackage[usenames]{color}
\usepackage{colordvi, xcolor}
\usepackage{pdfsync}
\usepackage{amsfonts}

\usepackage{color}
\usepackage{appendix}

\setlength{\topmargin}{-0.0in}
\setlength{\textheight}{8.2in}
\setlength{\oddsidemargin}{0.0in}
\setlength{\evensidemargin}{-0.0in}
\setlength{\textwidth}{6.5in}
\addtolength{\parskip}{2pt}
\addtolength{\itemsep}{0.1in}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{proposition}{Proposition}
\newtheorem{claim}{Claim} \newtheorem{lemma}{Lemma}
\newtheorem{fact}{Fact}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}{Observation}
\newtheorem{exercise}{Exercise}
\newtheorem{hypothesis}{Hypothesis}

\newcommand{{\Ex}}{\mathbb{E}}
\newcommand{{\TM}}{T^C}

\newcommand{{\C}}{\mathcal{C}}
\newcommand{{\Ob}}{\Omega}

\newcommand{{\bchords}}{Black$-$Chords}
\newcommand{{\wchords}}{White$-$Chords}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\ifx\pdftexversion\undefined
\usepackage[colorlinks,linkcolor=black,filecolor=black,citecolor=black,urlco
lor=black,pdfstartview=FitH]{hyperref}
\else
\usepackage[colorlinks,linkcolor=blue,filecolor=blue,citecolor=blue,urlcolor
=blue,pdfstartview=FitH]{hyperref}
\fi
\newcommand{\supp}{{\rm supp}}
\newcommand{\opt}{{\rm OPT}}
\newcommand{\disc}{{\rm disc}}
%\newcommand{\choos}[2] {\left( \begin{array}{c} #1\\#2





%\end{array} \right)}
\newcommand{\namedref}[2]{\hyperref[#2]{#1~\ref*{#2}}}
\newcommand{\sectionref}[1]{\namedref{Section}{#1}}
\newcommand{\factref}[1]{\namedref{Fact}{#1}}
\newcommand{\appendixref}[1]{\namedref{Appendix}{#1}}
\newcommand{\subsectionref}[1]{\namedref{Subsection}{#1}}
\newcommand{\theoremref}[1]{\namedref{Theorem}{#1}}
\newcommand{\defref}[1]{\namedref{Definition}{#1}}
\newcommand{\figureref}[1]{\namedref{Figure}{#1}}
\newcommand{\claimref}[1]{\namedref{Claim}{#1}}
\newcommand{\lemmaref}[1]{\namedref{Lemma}{#1}}
\newcommand{\tableref}[1]{\namedref{Table}{#1}}
\newcommand{\propref}[1]{\namedref{Proposition}{#1}}
\newcommand{\obref}[1]{\namedref{Observation}{#1}}
\newcommand{\eqnref}[1]{\namedref{Equation}{#1}}
\newcommand{\remarkref}[1]{\namedref{Remark}{#1}}
\newcommand{\corollaryref}[1]{\namedref{Corollary}{#1}}
\newcommand{\propertyref}[1]{\namedref{Property}{#1}}
%\input{psfig}

\newcommand{\alert}[1]{\textbf{\color{red}
[[[#1]]]}\marginpar{\textbf{\color{red}**}}\typeout{ALERT:
\the\inputlineno: #1}}

\newenvironment{cproof}
{\begin{proof}
 [Proof.]
 \vspace{-1.5\parsep}%-3.2\parsep} %%%For use with US letter
}
{\renewcommand{\qed}{\hfill $\Diamond$} \end{proof}}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\newcommand{\qed}{\hspace*{\fill}\mbox{$\Box$}}
\newcommand{\choos}[2] {\left( \begin{array}{c} #1\\#2
\end{array} \right)}



%\input{psfig}

\begin{document} \bibliographystyle{alpha}
\def\proofend{\hfill$\Box$\medskip}
\def\Proof{\noindent{\bf Proof:\ \ }}
\def\Sketch{\noindent{\bf Sketch:\ \ }}
\def\eps{\epsilon}
\newcommand{\arash}[1]{\noindent{\bf {\color{blue!60!black}{\sc Arash:}  #1}}}
\newcommand{\cindy}[1]{\noindent{\bf {\color{magenta!80!black}{\sc Cindy:}  #1}}}
\newcommand{\response}[1]{\noindent{{\color{blue!80!black}  #1}}}
\iffalse
\renewcommand{\arash}[1]{}
\renewcommand{\cindy}[1]{}
\fi
\title{Responses to reviewers comments}
\maketitle


\section{General Response}
\response{We thank the editor and reviewers for their insightful and constructive comments. }


\arash{TODO: Thank the reviewers at length. Apologies for the delay in submission for the revisions, thanks the editor for the extensions.}
\section{Detailed Responses}


\subsection{Reviewer 1}
\subsubsection{Major Comments}
\response{We have addressed the issue in the proof of Theorem 3 as detailed below. We also made several major revision in our presentation specially with regards to the more technical parts of the paper.}
\subsubsection{Minor Comments}
\begin{itemize}
\item Introduction: Introduction jumps between describing methods and problems the methods are applied to. The there is a subsection 1.2 Experiments with more problem descriptions. Please restructure the introduction so
that the flow is more logical.
\response{We agree that the introduction is a little bit lengthy and not direct. However, we wanted to make sure that the reader is familiar with our terminology before diving into the results presentation.

That being said, we have made some changes in the introduction. We hope the new state of the introduction is satisfactory.}


\item p. 2, last paragraph: “Integrality gap” is defined with respect to an IP instance, instead of a family of IP instances (IP formulation) for a problem. This makes the paragraph rather awkward to read, because the statements are not correct as stated.
\response{We made various changes starting from the paragraph before Definition 1 and the discussion that follows, drawing the comparison between different objects that a gap can be defined on and where Definition 1 stands. Our definition is integrality gap of a polyhedron.}

\item p. 3, below equation (1): Why is $A$ defined as a matrix of rational numbers but $b$ a vector or real numbers?

\response{Fixed. Both $A$ and $b$ are comprised of rational numbers.}
\item p. 3: Is there a reason why you choose to not include the coefficients of the objective function in an instance?
\response{Added this sentence: The reason that we define integrality gap for a polyhedron as opposed to an instance of a problem is that our algorithms are independent of the cost function $c$. }

\item p. 4, Definition 1: Now the integrality gap is defined with respect to a matrix $A$ and vector $b$, taking the supremum (not maximum, please correct) over all coefficients $c$, that are nonnegative. This is inconsistent
with the previous statement, and also nonstandard as far as I know. Please add some remarks for the reader about this choice. Also, the importance of considering $c \geq 0$ only should be emphasized more in the text (it is of
vital importance when using Carr-Vempala).

\response{As stated above, the discussion following Definition 1. We added a remark that $c$ need to be non-negative for Carr-Vempala to works.} 

\item p. 4, equation (4): supremum instead of maximum
\response{Fixed.}
\item p. 5, Proposition 2: Please specify where this is stated in [GLS93].
\item p. 5, below Proposition 2: I don’t see that Proposition 2 and Theorem 3 are equivalent as stated. (Also in light of the remark about one result having an algorithm, and the other not.)
\response{Fixed. Now we say they are similar statement. We also tried to word the statements in a way to draw the similarity between the two.}



\item p. 6, Theorem 4: “Moreover, [. . .]” — $C$ is not specified otherwise; replace “Moreover” by “where”.
\response{Fixed.}
\item p. 6, Theorem 5: Why “$1\leq$”? Is that not automatic?
\response{We added the lower bound as a reminder to the reader, but we removed since we agree that it is redundant.}
\item p. 6, below Theorem 5: “in iteration i the algorithm maintains a convex combination of vecotrs” — please phrase this more precisely 
\response{Done. The algorithm computes a convex combination from the one constructed in the last iteration that is progressively more integral.}
\item p. 6, near bottom: “It’s subtour-elimination...” — Its
\response{Fixed.}
\item p. 7, equation (5): sup instead of max
\response{Fixed.}
\item p. 7, Theorem 6: “Moreover, [. . .]” — $C$ is not specified otherwise; replace “Moreover” by “where”.
\response{Fixed.}
\item p. 9, last bullet point of Contribution summary: “can support theoretical work” — be more specific about what you did, and what it showed. 
\response{This sentence is now added to this bullet: In particular, we computationally prove an upper bound on small instances of 2EC with a specific structure.}
\item p. 9, Lemma 7: add $\tilde{x}$ right after “Given an integral vector”
\response{Fixed.}
\item p. 9, bottom: Isn’t it even easier? Is $\{1\}^n$ not a point in $\mathcal{D}(P)$ (provided
$P\neq \emptyset$)? \response{Correct, however, the proof would be simpler if restrict the solution to the support of $x$. That's why we use the round-up (i.e. the support of solution).}
\item p. 11, 3rd line below definitions of $LP^{(\ell)}$ and $IP^{(\ell)}$ : “By Theorem 3, there
exist [not exists] $\tilde{z}_i\in S$". In the statement of Theorem 3 this is $\mathcal{D}(S)$. Is
$S$ the same as $\mathcal{D}(S)$ here? If so explain.
(`) If not, can you still draw the conclusion that
$IP^{(\ell)}\neq \emptyset$?


\response{Thank you for pointing this out. We corrected to say $\tilde{z}^i\in \mathcal{D}(S)$. We also added this observation at the beginning of section 2 that is cross-reference in the proofs: Any point $x\in [0,1]^n$ that is a convex combination of points in $\mathcal{D}(S)$, is also a convex combination of points in $\mathcal{D}(S) \cap [0,1]^n$}
\item p. 11, third paragraph: Same problem as previous bullet point.
\response{Fixed with referencing the Observation above.}
\item p. 13, top of page: Same problem.
\response{Fixed with referencing the Observation above.}
\item p. 24 and following, references: missing capitalization in a few entries
\response{We fixed many instances of missing capitalization.}

\end{itemize}


\subsection{Reviewer 2}


\subsubsection{Major Comments}
The computational results seem quite weak to me. The classic Feasibility Pump heuristic is known to produce solutions of fairly poor quality; it is designed primarily to produce feasible solutions to potentially difficult MIPs
quickly. On TAP and 2EC, the authors compare to problem-specific approximation algorithms that are not particularly designed to produce high quality solutions either.
\response{Our comparison of FDT to feasibility pump is to main showcase a use case of the algorithm and a direction that could be possibly explored. We are not offering FDT as a replacement for FP. We will add remarks that make this clear to the reader.

With respect to Christofides, we mistakenly mentioned comparing FDT with Christofides but we in fact compare FDT with BOMC (Best of Many Christofides) - this has been fixed in the abstract and the paper. As presented in Figure 3, we don't settle for the worst-case upper bound of 3/2. Similarly, for TAP, FJ81 is the only rounding algorithm known for the solution of the CUT-LP. Again, we don't use the worst-case upper bound of 2 and use the instance specific gap provided by the algorithm. Also we remark that FDT is a generic algorithm that applies to any type of IP formulation, while BOMC and FJ81 only work for a certain type of formulation.}

\begin{enumerate}

\item The abstract is misleading in multiple ways. First, it says that the paper will give a constructive version of the Carr-Vempala theorem but does not mention the rather large loss in quality of $g$ to (potentially) $g^n$ . Secondly,
it says that “[Dom2IP] more quickly determines if an instance has an unbounded integrality gap” but really this algorithm will only sometimes provide proof that the integrality gap is unbounded; it is not guaranteed
to certify this. Lastly, it is claimed that Christofides’ algorithm is the “best previous approximation algorithm”. But Christofides is only best wrt. its theoretical approximation guarantee not its practical performance
which is what is compared in the paper. 


\response{We made several fixed in the abstract that addressed all the points mentioned above.}
\item The paper should mention the randomized metarounding algorithm by Carr and Vempala and compare it to the FDT algorithm. 
\response{Added this paragraph right before section 1.1: 
Carr and Vempala [CV00] showed if there an algorithm $\mathcal{A}$ that for $I\in \mathcal{I}$ and $c\geq 0$ finds $z\in S(I)$ with $cz\leq g'cx$ for any $x\in P(I)$, then $g'x$ can be \textbf{write} as a convex combination of points in $S(I)$. However, for many known problems there is a gap between the best known $g'$ and $g(\mathcal{I})$. }
\item In the proofs of Lemma 8 and Lemma 11, I do not see why it can be assumed that $\lambda_0,\lambda_1,\lambda_2>0$. I think one has to allow the case in which
the branching step returns just 1 or (in the case of 2EC) just 2 vectors.
\response{Yes. We corrected this in the statement of Lemma 9, 10, 12. Also in the description of the Algorithm 2.}
\item As noted before, I find the comparisons against the FP heuristic on VC
and the 2-approximation for TAP rather dubious. To show that the FDT
algorithm performs much better than the theoretical $g^{|supp(x)|}$ bound, just comparing to the initial fractional solution is all that is needed. On the other hand, if the goal is to show that the FDT algorithm works well as a
general purpose feasibility heuristic, then comparing to a FDT to a more modern variant of FP (e.g. as in the Feasibility Pump 2.0 paper) would be more reasonable.

\response{The comparisons with FP are not presented to claim that FDT to provide direct comparisons between the two algorithms, but to provide a baseline on the IG upper bounds provided by another LP-based algorithm. We want to emphasize that the scope of this work in mostly on the theoretical contributions, rather than experimental results.}

\end{enumerate}

\subsubsection{Minor Comments}
\begin{enumerate}

\item  The abstract says “Dom2IP” but the paper mostly uses “DomToIP”. 
\response{Fixed.}
\item In Theorem 3, I think $\mathcal{D}(S(I))$ should just be $S(I)$.
\response{NOT FINAL: The proof of this theorem only shows it for the dominant of $S(I)$ and not $S(I)$ itself, although for many problems of interest we have $S(I)=\mathcal{D}(S(I))$. }

\item On page 6, “which runs in polynomial time algorithm” should be “which runs in polynomial time”. 
\response{Fixed.}
\item On page 6, the use of $\theta_0$ and $\theta_1$ is not consistent with how $\gamma_0$ and $\gamma_1$ are
defined in Lemma 8. 
\response{Fixed and switched to $\gamma_0$ and $\gamma_1$ in the overview to be consistent with Lemma 8.}
\item In Theorem 6, is $E_x=\mbox{supp}(x)$? \response{Fixed. We added the definition of $E_x$ right before Theorem 6.}


\item On page 8, the wording “We use the idea of fundamental extreme point to create the hardest LP solutions to decompose.” seems to suggest that this is something that will be done in this paper.
\response{We changed the wording as follows: Known polyhedral structure makes it easier to study integrality gaps for such problems. In particular, for the subtour-elimination relaxation [CR98, BC11, CV04] introduced fundamental extreme points that create the ``hardest'' LP solutions to decompose.}.
\item On page 8, you refer to Figure 2 but this figure only shows up on page 23.

\response{The figure is moved to page 3.}
\item You mention that the Dom2IP algorithm can be used to prove that an IPs integrality gap is unbounded. How effective is this? \arash{need to think about this.}
\item Lemma 9 requires $\gamma_0 + \gamma_1 \leq 1$. Perhaps this should already be included
in Lemma 8.
\response{Fixed. Also added a constraint into the formulation of LPC that ensures $\lambda_0 + \lambda_1\leq 1$ and corresponding changes in the proof of the Claim.}
\item In the proof of Lemma 9, “We need to show that $\gamma_0\hat{x}^0_j + \gamma\hat{x}^1_j \leq gx'_j.$" has
an extra $g$ in it.
\response{Fixed.}
\item On page 13, “nodes in level $L_i$” should be “nodes in level $i$” and “points
in the leaves” should be just “leaves.” \response{Fixed.}
\item On page 24, “applying this tool . . . can be a tool” is redundant. \response{Fixed.}
\end{enumerate}

\subsection{Reviewer \#3}

\subsubsection{Main Comments}
\begin{itemize}
\item
Is your definition of integrality gap used elsewhere? If not, try to use a different word - maybe something like “objective-oblivious integrality gap”? The point is to make it clear that yes, this is something like integrality gap, but it does NOT match anything the reader has seen before.
\response{We agree that using a different term will provide much better clarity for future readers.  We now call our version the ``integrality gap of a polyhedron'' (IGP)} \cindy{TODO: update this if we change it.}

\arash{I actually don't agree with the reviewer here. They are saying that in their opinion for integrality gap you keep $c$ constant and take $\max$ over instances. This does not make sense to me. Because $c$ itself is dependent on the instance. }

\arash{Let's look at Definition 5.13 in SW book: The integrality gap of an integer program is the worst-case ratio over all instances of the problem of value of an optimal solution to the integer programming formulation to value of an optimal solution to its linear programming relaxation. So this is $\max_c \frac{cx:Ax\leq b, x\in \mathbb{Z}^n}{cx:Ax\leq b, x\in \mathbb{R}^n}$, which is inline with Definition 1.}
\cindy{Does the SW book actually give that math-based definition, where the max is only over c?  I don't have a copy and I don't see a SW book in our reference list right now. Or is that math your interpretation? As I said to Bob in a phone call, for a given combinatorial optimization problem, A,b, and c can change from instance to instance. For some problems, c is fixed (e.g. all 1).  But any problem that allows weighting of the combinatorial objects could have changes anywhere (perhaps we could add examples of that, because this reviewer is really concerned about this). If the SW book has this definition, then we should include a citation to that book when we define what we mean by integrality gap. We should probably also include the papers that this reviewer gave us. We (and SW if the math is as above) are still considering the integrality gap when the polyhedron is fixed, which is not over all instances of a problem in general. If the papers this reviewer cited consider everything fixed but b, that is also a restriction. That is ours and theirs are still a subset of the ``normal'' definition. I still think it will help reduce confusion if we call ours something more restrictive.  Is that OK?}
\arash{The definition in SW is just in words. There are two ways to read SW: 1. the instances of an IP over $A,b,c$, 2. the instances of an IP over all $c$.}
\arash{To me the second one makes so much more sense: since integrality gap can only be defined with respect to a formulation - so if you change the formulation you expect the integraltiy gap to change. Reviewer \#3's definition is not like that: consider matching: the formulation with only degree constraints has gap infinity, but formulation with degree constraints and odd set constraints has gap $1$. While in definition of integrality gap of reviewer \#3 you can say that the gap is changing.}
\arash{I do agree we should cite what the reviewer suggested, but not sure where it fits.}
\cindy{I agree that the integrality gap depends on the formulation. I think of a formulation the way they appear in technical papers. There are parameters and variables, an objective function and then families of constraints. The form of the objective and constraints is the formulation. But the values of a particular set of parameters, which includes things like graph topology for graph-based problems, identifies a specific instance. This supports what you call definition 1 from WS. This is why I suggested we spend a little time somewhere in the paper discussing problem formulations that fix c or fix A or fix b, or fix none of them, etc. Given the (definition 1) form, which comes from a mathematical formulation, researchers can prove an upper bound (for min problems) of $alpha$ (a constant or a function of the input size) by finding an algorithm that ``rounds'' an optimal LP solution to a feasible integer solution and proves the objective of that solution is no more than $\alpha$ times the optimal LP value. One can prove a lower bound of $\beta$ on the integrality gap for that formulation by finding an instance, or a family of instances, where the integrality gap is at least $\beta$. It's useful to prove large lower bounds, because it shows one has to add more constraints to get a good LP-based approximation algorithm for that problem. This paper gives a computational/experimental way to prove a lower bound on the integrality gap for a problem. It's reasonable to fix some parts of the formulation for experiments. For this paper, the polyhedron is fixed for a given test. It's convenient that WS include the same restriction. I pulled a copy of the book.  I will try to look at the context of that definition and see why they give it that way.  Wherever we have this discussion is where we should include the references that Reviewer 3 gave to us.  I thought we had included a lot of the logic I just added, but perhaps not, or not clearly.}

\response{
We added this explanation after Definition 1:

Consider an instance of a combinatorial optimization problem $\mathcal{P}$. We can characterize $\mathcal{P}$ by a set of feasible solutions in $S\in \mathbb{Z}_{\geq 0}^n$ and a cost function $c\in \mathbb{R}_{\geq 0}^n$. In many cases (see [VZ,SW]) the set $S$ can described as a set of integer points within a polyhedron $P$ (in fact the choice of $P$ need not be unique.) In this case, the integrality gap of $\mathcal{P}$ with respect to $P$ is $\min\{cx:x\in S\}/{\min\{cx:x\in P\}}$. Hence $g(I)$ in Definition 1 is equivalent to the worst-case integrality gap of all instances of a problem with respect to $P(I)$. 


The reason that we define integrality gap for a polyhedron as opposed to an instance of a problem is that our algorithms are independent of the cost function $c$. 
 
}
Relatedly, Eisenbrand and Shmonin \cite{eisenbrand} introduced an algorithm that finds the maximum gap between the optimal values of an IP and its LP relaxation over all realizations of vector $b$ (for which the IP is feasible). Their algorithm runs in polynomial-time given $n$ is fixed. In contrast, FDT always runs in time polynomial in $n$, but obtains an upper bound on the integrality gap as opposed to an exact value.
\item
Mention the existing work on integrality gaps and compare your results to it.
\response{Added a pointer to Carr Vemapala Meta-rounding and  Eisenbrand and Shmonin Fixed Dimension work right before section 1.1. and motioned how it isn related to our work. Thank you for this pointer.}
\item
Mention known work on obtaining integer feasible solutions. Is FDT the first algorithm which constructs a feasible solution if the instance has finite integrality gap? For which definitions of integrality gap can this be done in polynomial time, and with what quality of the resulting solution (wrt the integrality gap)? \response{See above.}
\item
Explain why FDT cannot be used to obtain a feasible solution attaining g(I, c) – is it because despite g(I, c) being finite, g(I) is infinite? or would FDT find some solution, but of worse value because g(I) is much larger than g(I,c), or can both scenarios happen?

\response{Thank you for pointing this out. We added a short discussion right after Theorem 5 to discuss the issue in a (slightly) different way:  A keen reader might ask why Theorem 5 does not imply that P = NP, as one iteratively find a feasible solution $x^*\in S(I)$, and add the constraint $x\leq cx^*-\epsilon$ for some small enough $\epsilon>0$ to $I$ until DomToIP fails to find a feasible solution. However, note that adding constraints to $I$ might result in $g(I)$ becoming infinite, in which case DomToIP would fail to find a feasible solution to the IP. }
\end{itemize}


\subsubsection{Detailed Comments}
\begin{itemize}
\item I’m attaching a file with line numbers. I strongly recommend for any future submission
using the lineno package, because it makes my job as a reviewer much easier. I will refer to
pairs (page number, line number) below.
\response{Thank you for the recommendation. We added line numbers to the document.}
\item (2,10) “we have formulated” – better to say “instances … are known” and give a reference, e.g. to MIPLIB; or if you have some particularly hard instances in mind that YOU have formulated, you can refer to them, but this should be a fairly widely known
fact. 
\response{MIPLIB citation added and the wording is changed as suggested.}\cindy{I actually did mean that we, researchers at Sandia National Laboratories, have formulated problems that these solvers cannot solve. This is because our problems are from national security, and don't always look like the kinds of problem that industry or academia solve. These problems are not published. One doesn't generally publish MIP formulations that don't solve (negative results are not interesting unless accompanied by positive results, or for famous open problems). We can add that to the response, but stick to MIPLIB in the paper if you like. However, {\bf Question}: Did you check the status of MIPLIB?  How many moderate-size problems are not solvable by commercial solvers? Do you have to go to more recent updates of MIPLIB, like MIPLIB 2010 and probably even more recent to find unsolved problems and are they still moderate-sized?}

\item (2,14) “the” $\rightarrow$ “these”?
\response{Fixed.}
\item (2,26) “from the problem” – what problem? Maybe “using special problem structure together with the result of an LP relaxation to obtain an integer feasible solution
whose value is at most C times the value of the LP optimum” or something like that.
\response{Paragraph reworded as follows: Many combinatorial optimization problems can be formulated as IPs. An {\em LP-based approximation algorithm} uses the LP relaxations to the IP formulation to find provably good approximate feasible integer solutions in polynomial time. At the highest level, this involves solving the LP relaxation, using special  problem structure to find a feasible integer solution, and proving that the objective value of the solution is no more than $C$ times worse than the bound from the LP relaxation. The approximation factor $C$ can be a constant or depend on the input parameters of the IP, e.g. $O(\log(n))$ where $n$ is the number of variables in the formulation of the IP (the dimension of the problem).}
\item (3,7) you say that a separation oracle should return a most violated constraint, but I don’t think this is the standard definition of e.g. Grötschel-Lovász-Schrijver – any violated constraint suffices to get polynomial convergence. 
\response{Fixed.}
\item (3,11) “it could not before” -> “they could not before”
\response{Fixed.}
\item (3,23) “ResearcherS”
\response{Fixed.}
\item (4,26) “We cannot hope to find…” – you’ve said this already. \response{The sentence was removed.}
\item (5,8) “express A vector” \response{Fixed.}
\item (5,10) Please define $[0,1]$ which appears in $[0,1]^k$ . It seems you mean the closed interval between 0 and 1, but it is very commonly also used to be the set $\{0,1\}$, so clarification is in order. \response{Added: , where $[0,1]^n = \{x\in \mathbb{R}^n, 0\leq x_i\leq 1 \mbox{ for } i\in[n]\}$ and $\{0,1\}^n = [0,1]^n \cap \mathbb{Z}^n$.}
\item (5,16) “of A blocking type” \response{Fixed.}
\item (5,23) “assuming” –> “under” \response{Fixed as suggested.}
\item (5,30) ”$S(I)\in \{0,1\}^n$ should be $\subseteq$ instead of $\in$ \response{Fixed.}
\item (6,1) delete “algorithm” \response{Fixed.}
\item (6,3) “it would be optimal” - what is “it”? \response{Rephrased as follows: If $C = g(I)$, FDT would have been optimal.}
\item (6,3) “we can only guarantee a factor of” I think it would be clearer to say “we can
only guarantee $C\leq g(I)$ ... ” \response{Fixed as suggested.}
\item (6,7) how does the $\min$ in $\min(Cx^*, {0,1}^n )$ work? Coordinate-wise, or?
\response{ Yes. But we removed this point-wise minimizing from the statement to make it simpler to state.}
\item (6,27) “It’s” –> “Its” 
\response{Fixed.}
\item (7,5) what is $\chi^{F_i}$ ? I suppose it is the characteristic vector of $F_i$ but you should state
this clearly. Also, what is $E_x$ ? is it the number of edges of $x$, that is, $\mbox{supp}(x)|$? Again
please state clearly.  \response{We added definition of $E_x$ right before Theorem 6 and $\chi$ in the footnote of this page.}
\item (7,13) “A subset of $U$ of $V$ ” – delete first “of”  
\response{Fixed.}
\item (7,18) define or comment on UG-hard \response{Added this in the footnote: UGC is short for Unique Game Conjecture and conjectured to be NP-hard [Khot2002]}
\item (7,19) “feasbility” –> “feasibility”
\response{Fixed.}
\item (7,20) “3-5x slower” use “$\times$” (\\times) instead of “x”\response{Fixed.}
\item (7,29) Large space before “Note” - something fishy is going on here. \response{Fixed.}
\item (9,14) add your experimental results about VC here also 
\response{Added.}
\item (10,7) again, large space
\response{Fixed.}
\item (10,14) it seems to me that the definition of the coordinates of $x^{(\ell+1)}$ other than $\ell+1$
should appear in the algorithm too, not just here.
\response{Line 3 in Algorithm 1 takes care of the other coordinates.}
\item (11,14-25) I’m getting tripped up at line 17. I understand the case for coordinates which have $x_j=0$. But where do you deal with those that do not? I suppose I’m just
missing something obvious, but so could any other reader. Please clarify your proof here.
\response{Here the goal is to establish that there is a $z\leq x^{(\ell+1)}$. For the coordinates of $x^{(\ell+1)}$ that are zero we show the proof. The other coordinates of $x^{(\ell+1)}$ are 1. So we just need to emphasize that all the $\tilde{z}_i \leq 1$. This is added now, an a reference to an observation we added (Observation 7)}.

\item (12,10) “maintainS”  \response{Fixed.}
\item (13,28) “Growing and Pruning THE FDT tree”
\response{Fixed.}
\item (14,12) “It is easy to check” <– you do this quite a bit. Please go through your paper and
pay attention to all the “easy to check”, “obvious”, “clearly” etc. If these things really are easy, then just state them and don’t waste space. However, if some observation is necessary, think about the clearest and most concise way to impart this observation to the reader, and then do that instead of telling the reader that it should be “easy”. 

\response{Thank you for the reminder. We find several occurrences of trivially, obviously, easy to check and clearly.}

\item (14,12) “THE set $L'$” \response{Fixed.}
\item (14,25) “From THE leaves” 
\response{Fixed.}
\item (16,11-17) You should discuss why the intuitive approach of binarizing an LP (just split each variable into two binary variables and copy the respective column of the
constraint matrix) does not lead to the same or better result. Or maybe it does?
\response{We added this paragraph in the discussion of why $\{0,1,2\}$ problems are different from binary problems: Another approach to a $\{0,1,2\}$ problem could be to ``binarize'' the variables, but splitting each variable into two binary copies and solving the binary problem via FDT for binary IPs (Theorem 4). One potential issue that can arise by doing this is that the binary version of a problem might have a larger integrality gap compared to the one that allows larger multiplicity. One example of this is the 2EC problem considered in this section. As mentioned above, we have $g(\mbox{2EC})\leq \frac{3}{2}$, while $g(\mbox{2ECSS})\geq \frac{3}{2}$, where 2ECSS is a problem equivalent to 2EC but without any doubled edges allowed {CKKK08}.}

\item (19,14) “1Mb cache” – I looked up this CPU and it has 1MB of L2 cache. Note that
1Mb $\neq$ 1MB, and “cache” is quite an ambiguous term (there are several types of CPU
cache). Moreover, the amount of cache is a property of the CPU and is usually not
stated, so I would just not comment on it. If you must, give a correct capacity and
indicate the level of the cache. \response{We removed the cache amount. Thanks.}
\item (19,22-26) Is there no good benchmark set for TAP? If not, please mention that as
the reason why you study random instances. (It is not good practise in experimental
algorithmics to study random instances unless necessary.) \response{Unfortunately we couldn't find any benchmark database of instances for TAP. We can remove this experiment for the paper (or move it to an appendix).} 
\item (20,22) One idea to consider and comment on: did you warm-start each computation of FDT? It seems to me that you could reuse the model that FDT solves in each
iteration – basically, you only change the objective and fix the variables which corresponds to changing lower/upper bounds, so you should be able to save time on
building / destroying the model. I’m not sure how much the pruning model can be reused. Either way, please comment on this idea. The idea was successful elsewhere,
e.g. https://arxiv.org/abs/1911.08636 

\response{Thank you for the great suggestion. We added a comment related to this in the conclusion. Even-though this seems like a great suggestion we think trying this in our experiments would fall beyond the mostly theoretical scope of this work.}

\item (21,10) Christofides’ algorithm is no longer the fastest https://arxiv.org/abs/2007.01409

\response{We added a reference to the KKO paper in the footnote.}
\item (21,10-11) “In the classic graph-based…” <– this is a weird sentence; you should say
that you will re-cap the algorithm and then you can describe it. Right now it comes
out of nowhere.
\response{Rewrote this part: Let us remind our reader the Christofides algorithm for TSP: given a graph with edge costs find a minimum spanning tree of the graph. 
Next, we add an $O$-join, where $O$ is the set of odd-degree vertices in the spanning tree. Note that the solutions to the TSP are also 2-edge-connected, which means that Christofides algorithm also provides solutions to the 2EC.
The best-of-many Christofides (BOMC) algorithm~[AKS15] works better than the classic Christofides algorithm in practice[GW17].}


\end{itemize}

\end{document}