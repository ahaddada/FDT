\section{Introduction}

Mixed-integer linear programming (MILP), the optimization of a linear objective function subject to linear and integrality constraints, models many practical optimization problems including scheduling, logistics and resource allocation.  The set of feasible points for a MILP is the set
\begin{equation}
S(A,G,b)= \{(x,y)\in \mathbb{Z}^{n}\times \mathbb{R}^p\;:\; Ax+Gy\geq b\}  \label{S}.
\end{equation}
If we drop the integrality constraints, we have the linear relaxation of set $S(A,G,b)$,
\begin{equation}
P(A,G,b) = \{(x,y)\in \mathbb{R}^{n+ p}\;:\; Ax+Gy\geq b\}. \label{P}
\end{equation}

Let $I=(A,G,b)$ be the feasible set of a specific instance. Then $S(I)$ and $P(I)$ denote $S(A,G,b)$ and $P(A,G,b)$, respectively. Given a linear objective function $c$, a Mixed-Integer-Linear Program (MILP) is $\min \;\{cx:\; (x,y) \in S(I)\}$. It is  NP-hard even to determine if an MILP instance has a feasible solution~\cite{GJ79}. However, intelligent branch-and-bound strategies allow commercial and open-source MILP solvers to give exact solutions (or near-optimal with provable bound) to many specific instances of NP-hard combinatorial optimization problems. 

Relaxing the integrality constraints gives the polynomial-time-solvable linear-programming relaxation: $\min \;\{cx:\;(x,y)\in P(I) \}$.  The optimal value of this linear program (LP), denoted $z_{LP}(I,c)$, is a lower bound on the optimal value for the MILP, denoted $z_{IP}(I,c)$. The solution can also provide some useful global structure, even though the fractional values are not directly meaningful. {\em LP-based approximation algorithms} for combinatorial problems involve modeling the problem as a MILP, solving the LP relaxation, finding a (problem-specific) integer-feasible solution from the LP solution, and proving an approximation bound by comparing the solution value to the LP lower bound.

Many researchers (see \cite{vazirani,sw}) have developed polynomial-time LP-based algorithms that find solutions for special classes of MILPs whose cost are provably smaller than $C\cdot z_{LP}(I,c)$. The approximation factor $C$ can be a constant or depend a parameter of the MILP, e.g. $O(\log(n))$. However, for many combinatorial optimization problems there is a limit to such techniques. Define the \textit{integrality gap} of the MILP formulation for instance $I$ to be $g_I= \max_{c\geq 0}\frac{z_{IP}(I,c)}{z_{LP}(I,c)}$. This value depends on the constraints in (\ref{S}).  We cannot hope to find solutions for the MILP with objective values better than $g_I\cdot z_{LP}(I,c)$. 

More generally we can define the integrality gap for a class of instances $\mathcal{I}$:% In this case, the integrality gap for problem $\mathcal{I}$ is
\begin{equation}
g_\mathcal{I} = \max_{c\geq 0 , I\in\mathcal{I}}\frac{z_{IP}(I,c)}{z_{LP}(I,c)}
\end{equation}
For example, finding a  minimum-weight 2-edge-connected multigraph has a natural formulation: every cut is crossed at least twice.  The gap for this formulation is at most $\frac{3}{2}$ \cite{Wolsey1980} and at least $\frac{6}{5}$ \cite{carr-ravi}. Therefore, we cannot hope to obtain an LP-based $(\frac{6}{5}-\epsilon)$-approximation algorithm for this problem using this LP relaxation.

\paragraph{The value of good MILP formulations:} There can be multiple correct MILP formulations for a problem with different integrality gaps. Finding MILP formulations with small integrality gap, e.g. by adding extra contraints, enables better provable approximation algorithms.  Such formulations are also likely to work better in practice when using exact solvers because branch-and-bound algorithms for MILP use LP bounds to prove whole regions of the search space can be pruned. In this paper, we provide tools to help modelers develop MILP formulations with integrality gaps closer to the optimal.

\paragraph{Decomposition} Our methods apply theory connecting integrality gaps to sets of feasible solutions. Instances $I$ with  $g_I=1$ has $P(I)=\conv(S(I))$, the convex hull of the lattice of feasible points. In this case, $P(I)$ is an \textit{integral} polyhedron. The spanning-tree polytope and the perfect-matching polytope \cite{schrijver} have this property. For such problems there is an algorithm to express vector $x\in P(I)$ as a convex combination of points in $S(I)$ in polynomial time \cite{cons-cara}.
\begin{proposition}\label{cara}
	If $g_I=1$, then for $(x,y)\in P(I)$, there exists $\theta \in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i =1$ and $(\tilde{x}^i,\tilde{y}^i)\in S(I)$ for $i=1,\ldots,k$ such that $\sum_{i=1}^{k}\theta_i \tilde{x}^i\leq x$. Moreover, we can find such a convex combination in polynomial time.
\end{proposition}

Carr and Vempala~\cite{CV} gave a decomposition result for integrality gap $1<g(I)<\infty$. %A analogous result to Proposition \ref{cara} is the following theorem due to Carr and Vempala \cite{CV}, which
This is a generalization of Goemans' proof for blocking polyhedra \cite{goemans}. 

\begin{theorem}[Carr, Vempala \cite{CV}] \label{CV}
	Let $(x,y)\in P(I)$, there exists $\theta \in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i =1$ and $(\tilde{x}^i,\tilde{y}^i)\in \dom(S(I))$ for $i=1,\ldots,k$ such that $\sum_{i=1}^{k}\theta_i \tilde{x}^i\leq Cx$ if and only if $g_I \leq C$.
\end{theorem}
Here $\dom(P(I))$ is the set of points $(x',y')$ such that there exists a point $(x,y)\in P$ with $x'\geq x$, also known as the dominant of $P(I)$. For covering problems the polyhedron is essentially the same as its dominant, but this is not true in general. While there is an exact algorithm for problems with gap $1$, Theorem~\ref{CV} is existenial, with no construction.
%In contrast to Proposition \ref{cara} which implies exact algorithms for problems with a gap of 1, Theorem \ref{CV} does not imply an approximation algorithm, since it does not suggest how to find such a convex combination in polynomial time.
%This points to an interesting open question. 
% We show later, for $I$ with $g(I)<\infty$, the notation of dominant is in fact useful.
\iffalse

\begin{question*}\label{question1}
	Assume reasonable complexity assumptions (such as UGC or $\textrm{P}\neq \textrm{NP}$). Given instance $I$ with $1<g_I<\infty$ and $(x,y)\in P(I)$, can we find $\theta \in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i =1$ and $(\tilde{x}^i,\tilde{y}^i)\in \dom(\conv(S(I)))$ for $i=1,\ldots,k$ such that $\sum_{i=1}^{k}\theta_i \tilde{x}^i\leq g_Ix$ in polynomial time?
\end{question*}

This seems to be a very hard question. A more specific question is of more interest.

\begin{question}\label{question2}
	Assume reasonable complexity assumptions, a specific problem $\mathcal{I}$ with  $1<g_{\mathcal{I}}<\infty$, and $(x,y)\in P(I)$ for some $I\in \mathcal{I}$, can we find $\theta \in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i =1$ and $(\tilde{x}^i,\tilde{y}^i)\in S(I)$ for $i=1,\ldots,k$ such that $\sum_{i=1}^{k}\theta_i \tilde{x}^i\leq g_{\mathcal{I}}x$ in polynomial time?
\end{question}
Although Question \ref{question2} is wide open, for some problems there are polynomial time algorithms closing the gap. For example, for generalized Steiner forest problem \cite{jain} gave an LP-based 2-approximation algorithm. The gap for this problem is also lower bounded by 2. Same holds for the set covering problem \cite{randomizedrounding}. In fact, for set cover the approximation algorithm achieving the same factor as the integrality gap lower bound, is a \textit{randomized rounding} algorithm. Raghavan and Thompson \cite{randomizedrounding} showed that this technique achieves provably good approximation for many combinatorial optimization problems.  

If we relax Question \ref{question1} (resp. Question \ref{question2}), but multiplying $g(I)$ (resp. $g(\mathcal{I})$) by a factor $C$, they are still very interesting, since they will provide upper bounds on the integraltiy gap the instance (resp. the problem). The results in this paper serve this purpose.
\fi
To study integrality gaps, we wish to find such a solution contructively:
\begin{question}\label{question2}
	Assume reasonable complexity assumptions, a specific problem $\mathcal{I}$ with  $1<g_{\mathcal{I}}<\infty$, and $(x,y)\in P(I)$ for some $I\in \mathcal{I}$, can we find $\theta \in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i =1$ and $(\tilde{x}^i,\tilde{y}^i)\in S(I)$ for $i=1,\ldots,k$ such that $\sum_{i=1}^{k}\theta_i \tilde{x}^i\leq $C$ g_{\mathcal{I}}x$ in polynomial time? We wish to find the smallest slack factor $C$ as possible.
\end{question}

We give a general approximation framework for solving $\{0,1\}$-MILPs.  Consider the set of point described by sets $S(I)$ and $P(I)$ as in (\ref{S}) and (\ref{P}), respectively. Assume in addition that $S(I),P(I)\subseteq [0,1]^n\times \mathbb{R}^p$. For a vector $x\in \mathbb{R}_{\geq 0}^n$ such that $(x,y)\in P(I)$ for some $y\in \mathbb{R}^p$, let $\spp(x)= \{i \in \{1,\ldots,n\}: x_i \neq 0\}$. 


\textit{Fractional Decomposition Tree} (FDT) is a polynomial-time algorithm that given a point $(x,y)\in P(I)$ produces a convex combination of feasible points in $S(I)$ that are dominated by a ``factor" $C$ of $x$ in the coordinates corresponding to $x$. If $C = g_I$, it would be optimal. However we can only guarantee a factor of $g_I^{|\spp(x)|}$. FDT relies on iteratively solving linear programs that are about the same size as the description of $P(I)$.

\begin{restatable}{theorem}{binaryFDT}
	\label{binaryFDT}
	Assume $1\leq g_I 	<\infty$. 	
	The Fractional Decomposition Tree (FDT) algorithm, given $(x^*,y^*)\in P(I)$, produces in polynomial time $\lambda\in [0,1]^k$ and $(z^1,w^1),\ldots,(z^k,w^k) \in S(I)$ such that $k\leq |\spp(x^*)|$, $\sum_{i=1}^{k}\lambda_i z^i\leq Cx^*$, and $\sum_{i=1}^{k}\lambda_i = 1$. Moreover, $C\leq g_I^{|\spp(x^*)|}$.
\end{restatable}

FDT finds feasible solutions to any MILP with finite gap. This can be of independent interest, especially in proving that a model has unbounded gap.
\begin{restatable}{theorem}{DomToIP}
	\label{domtoIP}
	Assume $1\leq g_I < \infty$. The DomToIP algorithm finds $(\hat{x},\hat{y})\in S(I)$ in polynomial time.
\end{restatable}

For general $I$ it is NP-hard to even decide if $S(I)$ is empty or not. There are a number of heuristics for this purpose, such as the feasibility pump heuristic \cite{fp1,fp2}. These heuristics are often very effective and fast in practice, however, they can sometimes fail to find a feasible solution. These heuristics do not provide any bounds on the quality of the solution they find. 

We consider the following TSP-related problems.  The {\em 2-edge-connected subgraph problem (2EC)} is to find a minimum-weight 2-edge-connected multigraph (subgraph which can contain multiple copies of each edge)
in a graph $G=(V,E)$ with respect to weights $c\in \mathbb{R}^E_{\geq 0}$. In the {\em tree-augmentation problem (TAP)} we wish to add a minimum-cost set of edges to a tree to make it 2-edge-connected.  We formally define TAP in Section~\ref{experiment}.

One can extend the FDT algorithm for binary MILPs into covering $\{0,1,2\}$-MILPs by losing a factor $2^{|\spp(x)|}$ on top of the loss for FDT. In order to eradicate this extra factor, we need to treat the coordinate $i$ with $x_i=1$ differently. For 2EC we are able to achieve this. The 2EC problem has the natural linear programming relaxation is
\begin{equation}\label{2ECpol}
\EC(G)= \{x\in [0,2]^{E}\;:\; x(\delta(U))\geq 2 \text{ for $\emptyset \subset U\subset V$})\}.
\end{equation}
 We prove the following theorem.

\begin{restatable}{theorem}{FDTEC}
	\label{FDT2EC}
	Let $G=(V,E)$ and $x$ be an extreme point of  $\EC(G)$. The FDT algorithm for 2EC produces $\lambda\in [0,1]^k$ and 2-edge-connected multigraphs $F_1,\ldots,F_k$ such that $k\leq 2|V|-1$, $\sum_{i=1}^{k}\lambda_i \chi^{F_i}\leq Cx$, and $\sum_{i=1}^{k}\lambda_i = 1$. Moreover, $C\leq g_{\EC}^{k}$, where $g_{\EC}$ is the integrality gap of the 2-edge-connected multigraph problem with respect to the formulation in (\ref{2ECpol}). 
\end{restatable}

We give a stronger characterization of integrality gap than that in Theorem \ref{CV} for bounded covering problems. For this purpose assume $P= \{x\in \mathbb{R}^n_{\geq 0}: Ax\geq b\cdot \textbf{1}, x \leq b\cdot \textbf{1}\}$, where $A\in \mathbb{Z}^{m\times n}_{\geq 0}$ and $b\in \mathbb{Z}_{\geq 0}$. Let $S= P\cap \mathbb{Z}^n$ and $g= \max_{c\geq 0} \frac{\min_{x\in S}cx}{\min_{x\in P}cx}$. Examples of problems whose natural linear programming relaxation is $P$ (for some matrix $A$ and integer $b$) include the 2-edge-connected multigraph problem, Steiner tree problem, tree augmentation problem, and many others.

\begin{restatable}{theorem}{tightcuts}
	\label{tightcuts}	
	We have $g\leq C$ if and only if for each extreme point $x$ of $P$, there exists $\theta\in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i = 1$ and $\tilde{x}^i \in S$ for $i=1,\ldots,k$ such that \begin{itemize}
		\item for $\ell\in \{1,\ldots,n\}$, if $x_\ell =0$, then $\tilde{x}^i_\ell=0$ for $i =1,\ldots,k$, i.e. $\tilde{x}^i$ is in the support of $x$,
		\item we have $A_j (\sum_{i=1}^{k}\theta_i\tilde{x}^i)\leq C\cdot A_j x$, for $j$ such that $A_j x =b$.
	\end{itemize}
\end{restatable}



This means in order to prove an upper bound on the integrality gap of a  bounded covering problem, we need to show there is a convex combination of integer feasible points that is ``cheap'' on all tight cuts. Notice that Theorem \ref{CV} requires the certificate convex combination to be ``cheap" on every single variable.




\paragraph{Experiments} Although the bound guaranteed in both Theorems \ref{binaryFDT} and \ref{FDT2EC} are very large for large problems, we show that in practice, the algorithm works very well for the TSP-like problems described above. We show how one might use FDT to investigate the integrality gap for such well-studied problems.

Known polyhedral structure makes it easier to study integrality gaps for such problems. Carr and Ravi \cite{carr-ravi} introduced fundamental extreme points. A point $x$ in Held-Karp relaxation for TSP (or 2EC; they have the same relaxation) is a point whose support of $x$, namely $G_x$ satisfies the following: i)  $G_x$ is a cubic graph, ii) in $G_x$ there is exactly one edge with $x_e=1$ incident to each node iii) The fractional edges of $G_x$ form a Hamiltonian cycle.  We say a fundamental extreme point (FEP) is {\em order $k$} if there are $k$ nodes on this Hamiltonian cycle. An FEP of order $k$ could represent an instance with many more than $k$ vertices. Carr and co-authors~\cite{CV,carr-ravi,BC11} proved that showing that $Cx$ is a convex combination of tours (resp. 2-edge-connected multigraphs) for all fundamental extreme points is equivalent to proving that the integrality gap for TSP (resp. 2EC) is bounded above by $C$. We use fundamental extreme point to create the ``hardest'' LP solutions to decompose.

There are fairly good bounds for the integrality gap for TSP or $\EC$. Benoit and Boyd \cite{TSPcompute} used a quadratic program to show the integrality gap for TSP is at most $\frac{20}{17}$ for graphs with at most 10 vertices. Alexander et. al \cite{abe} used the same ideas to provide an upper bound of $\frac{7}{6}$ for $\EC$ on graphs with at most 10 vertices. For $\EC$ we show that the integrality gap is at most $\frac{6}{5}$ for FEPs of order at most 12. An FEP of order $k$ might correspond to an extreme point of a much bigger graph, since each edge in a FEP with value 1 actually corresponds to a path of edges with value 1.
%In fact, since FDT can be applied to different problem, we can use it to evaluate the integrality gap of other well-known problem.
For TAP, we create random fractional extreme points and round them using FDT. For the instances that we create the blow-up factor is always below $\frac{3}{2}$ providing an upper bound for such instances.

% providing approximation ratios that are far better than the theoretical bound in the theorem statements.
%We examine FDT for binary MILPs for problems such as the tree augmentation problem (TAP) and we apply FDT for 2EC on some interesting and ``hard to decompose" points in the linear relaxation. Our computational results show that the FDT algorithm is a good tool to evaluate the integrality gap of integer programming formulations. 



\paragraph{Contributions}  The paper has the following contributions: 
\begin{itemize}
\item We give a simple algorithm, DomToIP, that can prove a binary formulationâ€™s integrality gap is unbounded or if not provide a feasible integer solution.  Someone formulating a first MILP for a new problem can test it with DomToIP.  If the algorithm ever fails in finding a feasible solution, the MILP has an unbounded gap.
\item We give an algorithm, Fractional Decomposition Tree (FDT), to construct the convex decomposition in the Carr-Vempala theorem, perhaps scaling by a factor larger than the integrality gap.  Each step of this algorithm provides a {\em lower bound} on the instances integrality gap. This also provides a lower bound on the approximation factor of any LP-based approximation algorithm using this formulation. The overall approximation factor of the FDT algorithm is an upper bound on the integrality gap for that specific instance.
\item For a special set of problems related to TSP, where there is a notion of a fundamental extreme point and long-running attempts to exactly determine the integrality gap of classic formulations, experimental analysis with FDT can help give some intuition about which bound(s) is/are likely to be loose.  Computing on fundamental extreme points is a way to experimentally characterize the gap upper bound.  There is no guarantee. Still, this can help direct theoretical analysis in the most promising direction.  For instance for 2EC, FDT gives good approximate solutions, better than the best current competitor (Christofides' algorithm).

\item A stronger theoretical result on integrality gap that can be potentially employed to obtain improved upper bounds on the integrality gap of bounded covering problems.  
\end{itemize}


\iffalse{
Finally we give a stronger characterization of integrality gap than that in Theorem \ref{CV} for bounded covering problems. For this purpose assume $P= \{x\in \mathbb{R}^n_{\geq 0}: Ax\geq b\textbf{1}, x \leq b\textbf{1}\}$, $S= P\cap \mathbb{Z}^n$, and $g= \max_{c\geq 0} \frac{\min_{x\in S}cx}{\min_{x\in P}cx}$. Examples of such problems include the 2-edge-connected multigraph problem, the tree augmentation problem, and many others.


\begin{restatable}{theorem}{tightcuts}
	\label{tightcuts}
	Let $x\in P$, there exists $\theta\in [0,1]^k$, where $\sum_{i=1}^{k}\theta_i = 1$ and $\tilde{x}^i \in S$ for $i=1,\ldots,k$ such that \begin{itemize}
		\item for $\ell\in \{1,\ldots,n\}$, if $x_\ell =0$, then $\tilde{x}^i_\ell=0$ for $i =1,\ldots,k$, i.e. $\tilde{x}^i$ is in the support of $x$,
		\item we have $Cb = C\cdot A_j x \geq A_j (\sum_{i=1}^{k}\theta_i\tilde{x}^i)$, for $j$ such that $A_j x =b$,  
	\end{itemize}
	if and only if $C\geq g$.
\end{restatable}



This means in order to prove an upper bound on the integrality gap of a covering problem, we need to show there is a convex combination of integer feasible points that is ``cheap'' on all tight cuts. Notice that Theorem \ref{CV} requires the certificate convex combination to be ``cheap" on every single variable.
\subsection{Notations}
For vectors $x,y\in \mathbb{R}_{n}$ we say $x$ dominates $y$ if $x_i\geq y_i$ for $i= 1,\ldots,n$. For $m\times n$ matrix $A$, let $A_j$ be the $j$-th row of $A$ and $A^j$ be the $j$-th column of $A$. Let $\textbf{1}$ be the vector of all ones of a suitable dimension. For a set $S$ of vectors in $\mathbb{R}_{n}$, $\conv(S)$ is the convex hull of all the points in $S$.
}\fi
